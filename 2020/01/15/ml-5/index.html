<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>softmax多分类 | 阿瑜</title><meta name="description" content="softmax多分类"><meta name="keywords" content="机器学习,神经网络,前向传播,反向传播,softmax"><meta name="author" content="Yu"><meta name="copyright" content="Yu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2020/01/15/ml-5/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="softmax多分类"><meta name="twitter:description" content="softmax多分类"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="softmax多分类"><meta property="og:url" content="http://yoursite.com/2020/01/15/ml-5/"><meta property="og:site_name" content="阿瑜"><meta property="og:description" content="softmax多分类"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="商品价格预测" href="http://yoursite.com/2020/01/15/data-science-5/"><link rel="next" title="线性回归和逻辑回归" href="http://yoursite.com/2020/01/15/data-science-3/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'false',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#一-softmax反向传播推导"><span class="toc-number">1.</span> <span class="toc-text">一. softmax反向传播推导</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-softmax函数如下"><span class="toc-number">1.1.</span> <span class="toc-text">1. softmax函数如下</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-损失函数"><span class="toc-number">1.2.</span> <span class="toc-text">2. 损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-求解"><span class="toc-number">1.3.</span> <span class="toc-text">3. 求解</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-在softmax中对变量-p-yi-求导"><span class="toc-number">1.3.1.</span> <span class="toc-text">(1) 在softmax中对变量 $p_{yi}$ 求导</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-利用bp算法的思路"><span class="toc-number">1.3.2.</span> <span class="toc-text">(2) 利用bp算法的思路</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-对-f-k-函数求微分"><span class="toc-number">1.3.3.</span> <span class="toc-text">(3) 对 $f_k$ 函数求微分</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#二-实现"><span class="toc-number">2.</span> <span class="toc-text">二. 实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-定义层类"><span class="toc-number">2.1.</span> <span class="toc-text">1. 定义层类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-定义网络的类"><span class="toc-number">2.2.</span> <span class="toc-text">2. 定义网络的类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-基本数据集"><span class="toc-number">2.3.</span> <span class="toc-text">3. 基本数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#（1）首先设置隐藏层的激活函数为sigmoid"><span class="toc-number">2.3.1.</span> <span class="toc-text">（1）首先设置隐藏层的激活函数为sigmoid</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#（2）设置激活函数为relu"><span class="toc-number">2.3.2.</span> <span class="toc-text">（2）设置激活函数为relu</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-拓展数据集"><span class="toc-number">2.4.</span> <span class="toc-text">4. 拓展数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#（1）3-7划分"><span class="toc-number">2.4.1.</span> <span class="toc-text">（1）3 : 7划分</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#（2）5-5划分"><span class="toc-number">2.4.2.</span> <span class="toc-text">（2）5 : 5划分</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#（3）0-15-0-85划分"><span class="toc-number">2.4.3.</span> <span class="toc-text">（3）0.15 : 0.85划分</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#（4）加上正则惩罚项"><span class="toc-number">2.4.4.</span> <span class="toc-text">（4）加上正则惩罚项</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#（5）不加正则化项"><span class="toc-number">2.4.5.</span> <span class="toc-text">（5）不加正则化项</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#三-遇到的问题"><span class="toc-number">3.</span> <span class="toc-text">三. 遇到的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#（1）学习率过大"><span class="toc-number">3.1.</span> <span class="toc-text">（1）学习率过大</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#（2）batch-size为整个训练集大小"><span class="toc-number">3.2.</span> <span class="toc-text">（2）batch_size为整个训练集大小</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#（3）拟合结果"><span class="toc-number">3.3.</span> <span class="toc-text">（3）拟合结果</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/cover.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">阿瑜</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><div class="menu_mask"></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@2.0/cover/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item text-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">18</div></a></div></div><div class="mobile_data_item text-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">30</div></a></div></div><div class="mobile_data_item text-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">6</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-igloo"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-window-restore"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-stream"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-robot"></i><span> About</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">softmax多分类</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-01-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-02-06</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><ul>
<li>任务描述：</li>
<li><ul>
<li>在之前的神经网络作业基础上实现softmax进行多分类。不得调用机器学习框架中已有的实现，使用python实现softmax的前向与后向过程。</li>
</ul>
</li>
<li>基本数据集：</li>
<li><ul>
<li>（train.csv, test.csv）<br>在训练集上进行训练，并在数据集上进行预测。</li>
</ul>
</li>
<li>拓展数据集：</li>
<li><ul>
<li>（thyroid.txt）<br>将实现的模型在此数据集上训练（自己划分训练集，数据集），观察模型是否能够拟合此数据集，并设法提高模型的准确率。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="一-softmax反向传播推导"><a href="#一-softmax反向传播推导" class="headerlink" title="一. softmax反向传播推导"></a>一. softmax反向传播推导</h4><h5 id="1-softmax函数如下"><a href="#1-softmax函数如下" class="headerlink" title="1. softmax函数如下"></a>1. softmax函数如下</h5><p>$$<br>P_{i,k}=\frac{e^{f_{i,k}}}{\sum^m_{j=1}e^{f_{i,j}}}<br>$$</p>
<h5 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h5><p>$$<br>Loss =-log(P_{yi})<br>$$</p>
<h5 id="3-求解"><a href="#3-求解" class="headerlink" title="3. 求解"></a>3. 求解</h5><p>求：<br>$$<br>\frac{\partial L}{\partial X}\ \ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial L}{\partial W}\ \ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial L}{\partial b}<br>$$</p>
<h6 id="1-在softmax中对变量-p-yi-求导"><a href="#1-在softmax中对变量-p-yi-求导" class="headerlink" title="(1) 在softmax中对变量 $p_{yi}$ 求导"></a>(1) 在softmax中对变量 $p_{yi}$ 求导</h6><p>$$<br>P_{i,k}=\frac{e^{f_{i,k}}}{\sum^m_{j=1}e^{f_{i,j}}}看成一个整体<br>$$</p>
<p>所以对此函数求导<br>$$<br>\frac{\partial L_{i}}{\partial P_{yi}}=-\frac{1}{P_{yi}}<br>$$</p>
<h6 id="2-利用bp算法的思路"><a href="#2-利用bp算法的思路" class="headerlink" title="(2) 利用bp算法的思路"></a>(2) 利用bp算法的思路</h6><p>在 $P_{yi}$ 中对变量 $f_{x}$ 求导，其中的 $\sum_je^{f_i}=e^{f_1}+e^{f_2}+\cdots+e^{f_n}$。对其求偏导有两种情况</p>
<ul>
<li>$k=y_i$</li>
</ul>
<p>$$<br>\frac{\partial P_{yi}}{\partial f_{yi}}=\frac{e^{f_{yi}}\sum_je^{f_j}-e^{f_{yi}}*e^{f_{yi}}}{(\sum_je^{f_j})^2}=P_{yi}-(P_{yi})^2\\<br>结合\frac{\partial L_i}{\partial P_{yi}}=-\frac{1}{P_{yi}}可以得到\frac{\partial L_i}{\partial f_{yi}}=\frac{\partial L_i}{\partial P_{yi}}\frac{\partial P_{yi}}{\partial f_{yi}}=P_{yi}-1<br>$$</p>
<ul>
<li>$k\neq y_i$</li>
</ul>
<p>$$<br>\frac{\partial P_{yi}}{\partial f_k}=\frac{-e^{f_k}*e^{f_{yi}}}{\sum_j(e^{f_j})^2}=-P_{yi}*P_k\\<br>结合\frac{\partial L_i}{\partial P_{yi}}=-\frac{1}{P_{yi}}可以得到\frac{\partial L_i}{\partial f_k}=\frac{\partial L_i}{\partial P_{yi}}\frac{\partial P_{yi}}{\partial f_k}=P_k<br>$$</p>
<h6 id="3-对-f-k-函数求微分"><a href="#3-对-f-k-函数求微分" class="headerlink" title="(3) 对 $f_k$ 函数求微分"></a>(3) 对 $f_k$ 函数求微分</h6><p>$ f(x_i:W)=Wx_i+b$</p>
<p>在 $f_k$ 中对变量 $W、X、b$ 求导<br>$$<br>\frac{\partial f}{\partial x_i}=W\ \ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial f}{\partial W}=x_i\ \ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial f}{\partial b}=1\\<br>$$<br>结合前面的<br>$$<br>\frac{\partial L_i}{\partial f_{yi}}=\frac{\partial L_i}{\partial P_{yi}}\frac{\partial P_{yi}}{\partial f_{yi}}=P_{yi}-1\ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial L_i}{\partial f_k}=\frac{\partial L_i}{\partial P_{yi}}\frac{\partial P_{yi}}{\partial f_k}=P_k<br>$$</p>
<p>可以得到如下的梯度公式<br>$$<br>\frac{\partial L_i}{\partial W_{yi}}=x_i(P_{yi}-1)\ \ \ \ \ \ \ \ \ \ \ \ \ \frac{\partial L_i}{\partial W_{k}}=x_iP_k(k\neq y_i)\ \ \ \ \ \ \ \ \ \ \ \ \ dW=\frac{\partial L_i}{\partial W}=x_iP<br>$$<br>这里的<strong>P</strong>是经减1后得出的概率矩阵<br>$$<br>dx=\frac{\partial L_i}{\partial x_i}=PW\\<br>db=\frac{\partial L_i}{\partial b}=P<br>$$<br>所以现在就可以将这些梯度用于反向传播，与之前的神经网络结合起来求解。</p>
<h4 id="二-实现"><a href="#二-实现" class="headerlink" title="二. 实现"></a>二. 实现</h4><h5 id="1-定义层类"><a href="#1-定义层类" class="headerlink" title="1. 定义层类"></a>1. 定义层类</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_num, output_num, layer_name)</span>:</span></span><br><span class="line">        <span class="comment"># 该层的类别名字</span></span><br><span class="line">        self._layer_name = layer_name</span><br><span class="line">        <span class="comment"># 该层的节点数</span></span><br><span class="line">        self._unit_num = output_num</span><br><span class="line">        <span class="comment"># 初始化该层的权重参数</span></span><br><span class="line">        self._weights = np.random.rand(input_num, output_num)*<span class="number">0.01</span></span><br><span class="line">        <span class="comment"># self._b = np.random.rand(1, output_num) * np.sqrt(2 / input_num)</span></span><br><span class="line">        <span class="comment"># 初始化b</span></span><br><span class="line">        self._b = np.zeros((<span class="number">1</span>, output_num))</span><br><span class="line">        <span class="comment"># 输入该层数据的维度</span></span><br><span class="line">        self._input_num = input_num</span><br><span class="line">        <span class="comment"># 输出的维度</span></span><br><span class="line">        self._output_num = output_num</span><br><span class="line">        <span class="comment"># 初始化输入与输出为0</span></span><br><span class="line">        self._inputs = np.array([<span class="number">0.0</span>] * self._input_num)</span><br><span class="line">        self._outputs = np.array([<span class="number">0.0</span>] * self._output_num)</span><br></pre></td></tr></table></figure>

<p>其中层的类别有3个，分别为<code>softmax_layer</code>，<code>sigmoid</code>，<code>relu</code>。</p>
<p>层类对应的方法有如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_set_inputs</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">    <span class="comment"># 设置输入</span></span><br><span class="line">    self._inputs = inputs.copy()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_inputs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 取得该层的输入</span></span><br><span class="line">    <span class="keyword">return</span> self._inputs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_set_weights</span><span class="params">(self, weights)</span>:</span></span><br><span class="line">    <span class="comment"># 设置权重</span></span><br><span class="line">    self._weights = weights.copy()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_set_b</span><span class="params">(self, b)</span>:</span></span><br><span class="line">    <span class="comment"># 设置b</span></span><br><span class="line">    self._b = b.copy()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># shape = input * ouput</span></span><br><span class="line">    <span class="comment"># 获取一层的weights</span></span><br><span class="line">    <span class="keyword">return</span> self._weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_b</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 获取b</span></span><br><span class="line">    <span class="keyword">return</span> self._b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_outputs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 判断属于什么激活函数，然后代入对应的函数，计算最终输出结果</span></span><br><span class="line">    <span class="keyword">if</span> self._layer_name == <span class="string">'softmax_layer'</span>:</span><br><span class="line">        self._outputs = _softmax(np.dot(self._inputs,self._weights) + self._b)</span><br><span class="line">    <span class="keyword">elif</span> self._layer_name == <span class="string">'sigmoid'</span>:</span><br><span class="line">        self._outputs = _sigmoid(np.dot(self._inputs,self._weights) + self._b)</span><br><span class="line">    <span class="keyword">elif</span> self._layer_name == <span class="string">'relu'</span>:</span><br><span class="line">        self._outputs = _relu(np.dot(self._inputs,self._weights) + self._b)</span><br><span class="line">    <span class="keyword">return</span> self._outputs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update</span><span class="params">(self, delta_weight, delta_b)</span>:</span></span><br><span class="line">    <span class="comment"># 更新权重和b</span></span><br><span class="line">    self._weights = self._weights + delta_weight</span><br><span class="line">    self._b = self._b + delta_b</span><br></pre></td></tr></table></figure>

<h5 id="2-定义网络的类"><a href="#2-定义网络的类" class="headerlink" title="2. 定义网络的类"></a>2. 定义网络的类</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BPNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_num, hidden_num, output_num, random_seed=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 输入维度</span></span><br><span class="line">        self._input_num = input_num</span><br><span class="line">        <span class="comment"># 隐藏层节点数量</span></span><br><span class="line">        self._hidden_num = hidden_num</span><br><span class="line">        <span class="comment"># 输出维度</span></span><br><span class="line">        self._output_num = output_num</span><br><span class="line">        <span class="comment"># 隐藏层，激活函数为relu</span></span><br><span class="line">        self._hidden_layer = Layer(self._input_num, self._hidden_num, <span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># self._hidden_layer = Layer(self._input_num, self._hidden_num, 'sigmoid')</span></span><br><span class="line">        <span class="comment"># 输出层，softmax</span></span><br><span class="line">        self._output_layer = Layer(self._hidden_num, self._output_num, <span class="string">'softmax_layer'</span>)</span><br><span class="line">        <span class="comment"># 设置随机数种子，保证每次初始化权重的随机值相同</span></span><br><span class="line">        random.seed(random_seed)</span><br></pre></td></tr></table></figure>

<p>网络的方法有如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_layer_load_data</span><span class="params">(self, layer, batch_sample)</span>:</span></span><br><span class="line">    <span class="comment"># 给layer传递输入的值</span></span><br><span class="line">    layer._set_inputs(batch_sample)</span><br><span class="line">    <span class="comment"># 计算输出</span></span><br><span class="line">    <span class="keyword">return</span> layer._calculate_outputs()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_output</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">    <span class="comment"># 前向传播，计算输出结果</span></span><br><span class="line">    hidden_layer_outputs = self._layer_load_data(self._hidden_layer, sample)</span><br><span class="line">    output_layer_outputs = self._layer_load_data(self._output_layer, hidden_layer_outputs)</span><br><span class="line">    <span class="keyword">return</span> output_layer_outputs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_back_foward</span><span class="params">(self, batch_sample, batch_label, rate, lam)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对于输入的样本进行反向传播计算偏导</span></span><br><span class="line"><span class="string">    返回各项变化量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 样本数量</span></span><br><span class="line">    m = batch_sample.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># batch_size * layer_output_num</span></span><br><span class="line">    <span class="comment"># 计算每一层的输出</span></span><br><span class="line">    hidden_layer_outputs = self._layer_load_data(self._hidden_layer, batch_sample)</span><br><span class="line">    output_layer_outputs = self._layer_load_data(self._output_layer, hidden_layer_outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取每一层的权重</span></span><br><span class="line">    output_layer_weights = self._output_layer._get_weights()</span><br><span class="line">    hidden_layer_weights = self._hidden_layer._get_weights()</span><br><span class="line">    <span class="comment"># softmax层</span></span><br><span class="line">    <span class="comment"># shape: m * output_num</span></span><br><span class="line">    <span class="comment"># 计算减1过后的概率矩阵</span></span><br><span class="line">    dZ_softmax = (output_layer_outputs-batch_label) / m</span><br><span class="line">    <span class="comment"># 计算权重的变化量</span></span><br><span class="line">    delta_w = -rate * np.dot(hidden_layer_outputs.reshape(self._hidden_num, m), dZ_softmax)</span><br><span class="line">    <span class="comment"># 加上正则惩罚项</span></span><br><span class="line">    <span class="comment"># delta_w += -rate * lam/m*output_layer_weights</span></span><br><span class="line">    <span class="comment"># 计算b的变化量</span></span><br><span class="line">    delta_bw = -rate * np.sum(dZ_softmax,axis=<span class="number">0</span>,keepdims=<span class="literal">True</span>).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># relu</span></span><br><span class="line">    <span class="comment"># m * hidden_num</span></span><br><span class="line">    <span class="comment"># 隐藏层的偏差</span></span><br><span class="line">    dZ_hidden = _drelu(hidden_layer_outputs) * np.dot(dZ_softmax, output_layer_weights.transpose())</span><br><span class="line">    <span class="comment"># 计算隐藏层的权重的变化量</span></span><br><span class="line">    delta_v = -rate * np.dot(batch_sample.reshape(self._input_num, m), dZ_hidden)</span><br><span class="line">    <span class="comment"># 正则惩罚项</span></span><br><span class="line">    <span class="comment"># delta_v += -rate * lam/m*hidden_layer_weights</span></span><br><span class="line">    <span class="comment"># 计算b的变化量</span></span><br><span class="line">    delta_bv = -rate * np.sum(dZ_hidden,axis=<span class="number">0</span>,keepdims=<span class="literal">True</span>).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    # sigmoid</span></span><br><span class="line"><span class="string">    # shape: m * hidden_num</span></span><br><span class="line"><span class="string">    dZ_hidden = hidden_layer_outputs * (1 - hidden_layer_outputs) \</span></span><br><span class="line"><span class="string">        * np.dot(dZ_softmax, output_layer_weights.transpose())</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> delta_w, delta_bw, delta_v, delta_bv</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_train</span><span class="params">(self, samples, labels, samples_test, labels_test, rate=<span class="number">0.01</span>, epochs=<span class="number">100</span>, batch_size=<span class="number">2</span>, lam=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 整个训练集的数量</span></span><br><span class="line">    sample_num = samples.shape[<span class="number">0</span>]</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    best_acc1, best_acc2 = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="comment"># samples, labels = shuffle_data(samples, labels)</span></span><br><span class="line">    acc_train, acc_test, Loss = [],[],[]</span><br><span class="line">    <span class="comment"># 批量处理</span></span><br><span class="line">    mini_batches_samples = [</span><br><span class="line">        samples[k:k+batch_size]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, sample_num, batch_size)</span><br><span class="line">    ]</span><br><span class="line">    mini_batches_labels = [</span><br><span class="line">        labels[k:k+batch_size]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, sample_num, batch_size)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">while</span> count &lt;= epochs:</span><br><span class="line">        <span class="comment"># 按batch遍历</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(mini_batches_samples):</span><br><span class="line">            <span class="comment"># 反向传播更新权重</span></span><br><span class="line">            delta_w, delta_bw, delta_v, delta_bv = self._back_foward(batch, mini_batches_labels[i], rate, lam)</span><br><span class="line">            self._output_layer._update(delta_w, delta_bw)</span><br><span class="line">            self._hidden_layer._update(delta_v, delta_bv)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个epoch计算一次输出结果</span></span><br><span class="line">        <span class="comment"># 预测的概率</span></span><br><span class="line">        y_pre_train = self._forward_output(samples)</span><br><span class="line">        y_pre_test = self._forward_output(samples_test)</span><br><span class="line">        <span class="comment"># 交叉熵损失</span></span><br><span class="line">        loss = -np.sum(np.log(np.max(y_pre_train,axis=<span class="number">1</span>)))/sample_num</span><br><span class="line">        <span class="comment"># 记录整个loss</span></span><br><span class="line">        Loss.append(loss)</span><br><span class="line">        <span class="comment"># 计算训练集和测试集上的准确率</span></span><br><span class="line">        acc1 = calculate_acc(y_pre_train, labels)</span><br><span class="line">        acc2 = calculate_acc(y_pre_test, labels_test)</span><br><span class="line">        <span class="comment"># 每100个epoch输出</span></span><br><span class="line">        <span class="keyword">if</span> count % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: %d, loss: %.6f'</span> %(count, loss))</span><br><span class="line">            <span class="comment"># 更新最优的准确率</span></span><br><span class="line">            best_acc1 = max(acc1, best_acc1)</span><br><span class="line">            <span class="comment"># 保存测试集上的最优模型</span></span><br><span class="line">            <span class="keyword">if</span> acc2 &gt; best_acc2:</span><br><span class="line">                self.save(<span class="string">"myBPNN.m"</span>)</span><br><span class="line">                best_acc2 = acc2</span><br><span class="line">            <span class="comment"># 记录最优的值</span></span><br><span class="line">            acc_train.append(acc1)</span><br><span class="line">            acc_test.append(acc2)</span><br><span class="line">            print(<span class="string">'Epoch: %d, acc_train %.4f, acc_test %.4f'</span> % (count, acc1, acc2))</span><br><span class="line">            <span class="comment"># print('lr: %.6f' %rate)</span></span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 训练完毕</span></span><br><span class="line">    print(<span class="string">"\nTotal epochs: "</span> + str(count<span class="number">-1</span>))</span><br><span class="line">    print(<span class="string">'\nbest_acc1: %.4f, best_acc2: %.4f'</span> %(best_acc1,best_acc2))</span><br><span class="line">    <span class="keyword">return</span> np.array(acc_train), np.array(acc_test), np.array(Loss)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bpnn</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 打印bpnn的参数</span></span><br><span class="line">    hidden_layer_w = self._hidden_layer._get_weights()</span><br><span class="line">    hidden_layer_b = self._hidden_layer._get_b()</span><br><span class="line">    output_layer_w = self._output_layer._get_weights()</span><br><span class="line">    output_layer_b = self._output_layer._get_b()</span><br><span class="line">    print(<span class="string">"Hidden Layer:"</span>)</span><br><span class="line">    print(<span class="string">"\nWeights:\n"</span>, hidden_layer_w, <span class="string">"\nb:\n"</span>, hidden_layer_b)</span><br><span class="line">    print(<span class="string">"Output Layer:"</span>)</span><br><span class="line">    print(<span class="string">"\nWeights:\n"</span>, output_layer_w, <span class="string">"\nb:\n"</span>, output_layer_b)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">    <span class="comment"># 保存训练好的神经网络</span></span><br><span class="line">    model = &#123;</span><br><span class="line">        <span class="string">"hidden_layer_weights"</span>: self._hidden_layer._get_weights(),</span><br><span class="line">        <span class="string">"output_layer_weights"</span>: self._output_layer._get_weights(),</span><br><span class="line">        <span class="string">"hidden_layer_b"</span>: self._hidden_layer._get_b(),</span><br><span class="line">        <span class="string">"output_layer_b"</span>: self._output_layer._get_b()        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(model, f)</span><br><span class="line">    print(<span class="string">"Model Saved!"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">    <span class="comment"># 导入训练好的神经网络</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        model = pickle.load(f)</span><br><span class="line">        self._hidden_layer._set_weights(model[<span class="string">"hidden_layer_weights"</span>])</span><br><span class="line">        self._hidden_layer._set_b(model[<span class="string">"hidden_layer_b"</span>])</span><br><span class="line">        self._output_layer._set_weights(model[<span class="string">"output_layer_weights"</span>])</span><br><span class="line">        self._output_layer._set_b(model[<span class="string">"output_layer_b"</span>])</span><br><span class="line">    print(<span class="string">"Model loaded!"</span>)</span><br></pre></td></tr></table></figure>

<h5 id="3-基本数据集"><a href="#3-基本数据集" class="headerlink" title="3. 基本数据集"></a>3. 基本数据集</h5><p>数据集有train.csv和test.csv，已经划分好了训练集和测试集</p>
<p>总共有3类，训练集有4001条数据，测试集有500条数据，特征维度为2维。</p>
<p>其中训练集的分布图像如下<br><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/1.png" class="lozad"></p>
<p>测试集的分布图像如下</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/2.png" class="lozad"></p>
<p>可以看到训练集中有几个点交缠在一块。</p>
<h6 id="（1）首先设置隐藏层的激活函数为sigmoid"><a href="#（1）首先设置隐藏层的激活函数为sigmoid" class="headerlink" title="（1）首先设置隐藏层的激活函数为sigmoid"></a>（1）首先设置隐藏层的激活函数为sigmoid</h6><p>参数如下：</p>
<blockquote>
<p>input_num = 4001</p>
<p>hidden_num = 5</p>
<p>output_num = 3</p>
<p>激活函数: sigmoid</p>
<p>learing rate = 0.001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/3.png" class="lozad"></p>
<p>loss曲线如下，可以看到大概在400个epoch就接近收敛了</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/4.png" class="lozad"></p>
<p>训练集和测试集的准确率变化情况如下：</p>
<p>训练集：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/7.png" class="lozad"></p>
<p>测试集：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/5.png" class="lozad"></p>
<p>将测试集的准确率图像拉长过后如下：</p>
<p><img alt="6" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/6.png" class="lozad"></p>
<p>可以看到大概到100个epoch，就已经达到最优的结果了，测试集上的准确率为0.9980。</p>
<h6 id="（2）设置激活函数为relu"><a href="#（2）设置激活函数为relu" class="headerlink" title="（2）设置激活函数为relu"></a>（2）设置激活函数为relu</h6><p>参数如下：</p>
<blockquote>
<p>input_num = 4001</p>
<p>hidden_num = 5</p>
<p>output_num = 3</p>
<p>激活函数: sigmoid</p>
<p>learing rate = 0.0001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/12.png" class="lozad"></p>
<p>loss曲线如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/8.png" class="lozad"></p>
<p>训练集测试集的准确率变化情况如下：</p>
<p>训练集：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/9.png" class="lozad"></p>
<p>测试集：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/10.png" class="lozad"></p>
<p>拉长后的图像：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/11.png" class="lozad"></p>
<p>大概在40个epoch左右就已经达到最好的效果了，测试集上最好的准确率为0.998。可见relu激活函数实际上要比sigmoid激活函数好。</p>
<h5 id="4-拓展数据集"><a href="#4-拓展数据集" class="headerlink" title="4. 拓展数据集"></a>4. 拓展数据集</h5><p>数据集名称为thyroid.txt</p>
<p>训练集和测试集需要自己划分</p>
<p>总共有3类，7200条数据，特征维度为21维。</p>
<h6 id="（1）3-7划分"><a href="#（1）3-7划分" class="headerlink" title="（1）3 : 7划分"></a>（1）3 : 7划分</h6><p>参数如下：</p>
<blockquote>
<p>input_num = 5040</p>
<p>hidden_num = 20</p>
<p>output_num = 3</p>
<p>激活函数: sigmoid</p>
<p>learing rate = 0.0001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/19.png" class="lozad"></p>
<p>loss曲线：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/20.png" class="lozad"></p>
<p>训练集测试集准确率变化情况：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/21.png" class="lozad"></p>
<p>loss大概在200个epoch就基本收敛了，但是训练集和测试集的准确率从一开始就保持不变，测试集最优为0.9134。</p>
<h6 id="（2）5-5划分"><a href="#（2）5-5划分" class="headerlink" title="（2）5 : 5划分"></a>（2）5 : 5划分</h6><p>参数如下：</p>
<blockquote>
<p>input_num = 3600</p>
<p>hidden_num = 20</p>
<p>output_num = 3</p>
<p>激活函数: relu</p>
<p>learing rate = 0.0001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/16.png" class="lozad"></p>
<p>loss曲线如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/17.png" class="lozad"></p>
<p>测试集训练集的准确率变化情况：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/18.png" class="lozad"></p>
<p>loss大概在200个epoch就基本收敛了，但是训练集和测试集的准确率还是从一开始保持不变，测试集最优为0.9247。</p>
<h6 id="（3）0-15-0-85划分"><a href="#（3）0-15-0-85划分" class="headerlink" title="（3）0.15 : 0.85划分"></a>（3）0.15 : 0.85划分</h6><p>参数如下：</p>
<blockquote>
<p>input_num = 1080</p>
<p>hidden_num = 20</p>
<p>output_num = 3</p>
<p>激活函数: relu</p>
<p>learing rate = 0.0001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/13.png" class="lozad"></p>
<p>loss曲线如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/14.png" class="lozad"></p>
<p>训练集和测试集的准确率变化情况如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/15.png" class="lozad"></p>
<p>可以看到loss一直在下降，准确率还是不变。此时测试集准确率最优为0.9275。</p>
<h6 id="（4）加上正则惩罚项"><a href="#（4）加上正则惩罚项" class="headerlink" title="（4）加上正则惩罚项"></a>（4）加上正则惩罚项</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上正则惩罚项</span></span><br><span class="line">delta_w += -rate * lam/m*output_layer_weights</span><br><span class="line">delta_v += -rate * lam/m*hidden_layer_weights</span><br></pre></td></tr></table></figure>

<p>采用0.15 : 0.85的划分方式</p>
<p>参数如下：</p>
<blockquote>
<p>input_num = 1080</p>
<p>hidden_num = 20</p>
<p>output_num = 3</p>
<p>激活函数: relu</p>
<p>learing rate = 0.0001</p>
<p>epochs = 1000</p>
<p>batch_size = 2</p>
</blockquote>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/22.png" class="lozad"></p>
<p>最终训练出来的部分权重矩阵如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/23.png" class="lozad"></p>
<p><img alt="24" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/24.png" class="lozad"></p>
<p>可以看到很多节点的权重值非常小，不会产生梯度爆炸问题。</p>
<h6 id="（5）不加正则化项"><a href="#（5）不加正则化项" class="headerlink" title="（5）不加正则化项"></a>（5）不加正则化项</h6><p>和上面一样的参数，最终得到部分权重如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/25.png" class="lozad"></p>
<p><img alt="26" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/26.png" class="lozad"></p>
<p>此时结果就没有那么小了，此时如果学习率过大，就可能在反向传播的时候发生溢出，即梯度爆炸。</p>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/27.png" class="lozad"></p>
<p>测试集的准确率有轻微改变，所以是否加上正则化惩罚项可能会对结果产生影响。</p>
<h4 id="三-遇到的问题"><a href="#三-遇到的问题" class="headerlink" title="三. 遇到的问题"></a>三. 遇到的问题</h4><h5 id="（1）学习率过大"><a href="#（1）学习率过大" class="headerlink" title="（1）学习率过大"></a>（1）学习率过大</h5><p>刚开始在调整学习率的过程中，我把lr设为1，0.1，0.01，在结果出现了NaN的情况，发现是因为学习率过大，导致了计算梯度的时候结果太大，超出了exp的范围，梯度爆炸的问题。所以最终，我的学习率都不是很大。</p>
<h5 id="（2）batch-size为整个训练集大小"><a href="#（2）batch-size为整个训练集大小" class="headerlink" title="（2）batch_size为整个训练集大小"></a>（2）batch_size为整个训练集大小</h5><p>结果的准确率为0.33，反向传播失效，不知道什么原因，可能梯度消失了。而且batch_size越大，我的训练速度越快。在多次测试中我发现，batch_size越大，最终结果并没有小的时候好，所以我的batch_size基本都设置为2了。</p>
<h5 id="（3）拟合结果"><a href="#（3）拟合结果" class="headerlink" title="（3）拟合结果"></a>（3）拟合结果</h5><p>可能因为数据集太小，基本上都不需要1000个epoch就能达到最优结果，甚至只需要200个左右，但是loss是一直在下降。对于拓展数据集，可能因为分类为3的类别占了一大部分，导致最终结果看上去没有训练的过程。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/01/15/ml-5/">http://yoursite.com/2020/01/15/ml-5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">阿瑜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a><a class="post-meta__tags" href="/tags/神经网络/">神经网络    </a><a class="post-meta__tags" href="/tags/前向传播/">前向传播    </a><a class="post-meta__tags" href="/tags/反向传播/">反向传播    </a><a class="post-meta__tags" href="/tags/softmax/">softmax    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.png"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.png"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/15/data-science-5/"><img class="prev_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/5/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>商品价格预测</span></div></a></div><div class="next-post pull-right"><a href="/2020/01/15/data-science-3/"><img class="next_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/3/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>线性回归和逻辑回归</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/01/15/ml-4/" title="单隐层神经网络的推导及实现"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/cover.jpg"><div class="relatedPosts_title">单隐层神经网络的推导及实现</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-5/" title="商品价格预测"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/5/cover.jpg"><div class="relatedPosts_title">商品价格预测</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/ml-1/" title="MNIST数据集处理"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/1/cover.jpg"><div class="relatedPosts_title">MNIST数据集处理</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-3/" title="线性回归和逻辑回归"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/3/cover.jpg"><div class="relatedPosts_title">线性回归和逻辑回归</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-4/" title="梯度下降法"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/4/cover.jpg"><div class="relatedPosts_title">梯度下降法</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/ml-3/" title="FINDS算法及决策树ID3算法实现"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/3/cover.jpg"><div class="relatedPosts_title">FINDS算法及决策树ID3算法实现</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div id="footer"><div class="copyright">&copy;2018 - 2020 By Yu</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Welcome to my <a href="https://darkseidddddd.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></section><div class=" " id="post_bottom"><div id="post_bottom_items"><a id="mobile_to_comment" href="#post-comment"><i class="mobile_scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#一-softmax反向传播推导"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一. softmax反向传播推导</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-softmax函数如下"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">1. softmax函数如下</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-损失函数"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">2. 损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-求解"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">3. 求解</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#1-在softmax中对变量-p-yi-求导"><span class="toc_mobile_items-number">1.3.1.</span> <span class="toc_mobile_items-text">(1) 在softmax中对变量 $p_{yi}$ 求导</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#2-利用bp算法的思路"><span class="toc_mobile_items-number">1.3.2.</span> <span class="toc_mobile_items-text">(2) 利用bp算法的思路</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#3-对-f-k-函数求微分"><span class="toc_mobile_items-number">1.3.3.</span> <span class="toc_mobile_items-text">(3) 对 $f_k$ 函数求微分</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#二-实现"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">二. 实现</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-定义层类"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">1. 定义层类</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-定义网络的类"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">2. 定义网络的类</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-基本数据集"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">3. 基本数据集</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（1）首先设置隐藏层的激活函数为sigmoid"><span class="toc_mobile_items-number">2.3.1.</span> <span class="toc_mobile_items-text">（1）首先设置隐藏层的激活函数为sigmoid</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（2）设置激活函数为relu"><span class="toc_mobile_items-number">2.3.2.</span> <span class="toc_mobile_items-text">（2）设置激活函数为relu</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#4-拓展数据集"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">4. 拓展数据集</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（1）3-7划分"><span class="toc_mobile_items-number">2.4.1.</span> <span class="toc_mobile_items-text">（1）3 : 7划分</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（2）5-5划分"><span class="toc_mobile_items-number">2.4.2.</span> <span class="toc_mobile_items-text">（2）5 : 5划分</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（3）0-15-0-85划分"><span class="toc_mobile_items-number">2.4.3.</span> <span class="toc_mobile_items-text">（3）0.15 : 0.85划分</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（4）加上正则惩罚项"><span class="toc_mobile_items-number">2.4.4.</span> <span class="toc_mobile_items-text">（4）加上正则惩罚项</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-6"><a class="toc_mobile_items-link" href="#（5）不加正则化项"><span class="toc_mobile_items-number">2.4.5.</span> <span class="toc_mobile_items-text">（5）不加正则化项</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#三-遇到的问题"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">三. 遇到的问题</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（1）学习率过大"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">（1）学习率过大</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（2）batch-size为整个训练集大小"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">（2）batch_size为整个训练集大小</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（3）拟合结果"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">（3）拟合结果</span></a></li></ol></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>