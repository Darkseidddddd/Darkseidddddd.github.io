<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>单隐层神经网络的推导及实现 | 阿瑜</title><meta name="description" content="单隐层神经网络的推导及实现"><meta name="keywords" content="机器学习,神经网络,Hebb学习规则,前向传播,反向传播"><meta name="author" content="Yu"><meta name="copyright" content="Yu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2020/01/15/ml-4/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="单隐层神经网络的推导及实现"><meta name="twitter:description" content="单隐层神经网络的推导及实现"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="单隐层神经网络的推导及实现"><meta property="og:url" content="http://yoursite.com/2020/01/15/ml-4/"><meta property="og:site_name" content="阿瑜"><meta property="og:description" content="单隐层神经网络的推导及实现"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="梯度下降法" href="http://yoursite.com/2020/01/15/data-science-4/"><link rel="next" title="线性回归和逻辑回归" href="http://yoursite.com/2020/01/15/data-science-3/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'false',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#一-神经网络"><span class="toc-number">1.</span> <span class="toc-text">一. 神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#二-优化算法"><span class="toc-number">2.</span> <span class="toc-text">二. 优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-前向传播算法"><span class="toc-number">2.1.</span> <span class="toc-text">1. 前向传播算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-反向传播"><span class="toc-number">2.2.</span> <span class="toc-text">2. 反向传播</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#三-代码实现"><span class="toc-number">3.</span> <span class="toc-text">三. 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-隐藏层个数为10"><span class="toc-number">3.1.</span> <span class="toc-text">1. 隐藏层个数为10</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-隐藏层个数为30"><span class="toc-number">3.2.</span> <span class="toc-text">2. 隐藏层个数为30</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-隐藏层个数为100"><span class="toc-number">3.3.</span> <span class="toc-text">3. 隐藏层个数为100</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-隐藏层个数为300"><span class="toc-number">3.4.</span> <span class="toc-text">4. 隐藏层个数为300</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-学习率为0-3"><span class="toc-number">3.5.</span> <span class="toc-text">1. 学习率为0.3</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-学习率为0-03"><span class="toc-number">3.6.</span> <span class="toc-text">2. 学习率为0.03</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#四-Hebb学习规则"><span class="toc-number">4.</span> <span class="toc-text">四. Hebb学习规则</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/cover.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">阿瑜</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><div class="menu_mask"></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@2.0/cover/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item text-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item text-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">29</div></a></div></div><div class="mobile_data_item text-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-igloo"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-window-restore"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-stream"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-robot"></i><span> About</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">单隐层神经网络的推导及实现</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-01-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-02-06</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/数据科学导论/">数据科学导论</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><ul>
<li>推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程，探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。探讨不同学习率对迭代次数和网络性能的影响（使用MNIST数据集）。</li>
<li>Hebb学习规则</li>
</ul>
<hr>
<hr>
<h4 id="一-神经网络"><a href="#一-神经网络" class="headerlink" title="一. 神经网络"></a>一. 神经网络</h4><p>​    神经网络主要由：<strong>输入层</strong>，<strong>隐藏层</strong>，<strong>输出层</strong>构成。当隐藏层只有一层时，该网络为<strong>两层神经网络</strong>，即单隐层神经网络，由于输入层未做任何变换，可以不看做单独的一层。实际中，网络输入层的每个神经元代表了一个特征，输出层个数代表了<strong>分类标签</strong>的个数（在做二分类时，如果采用sigmoid分类器，输出层的神经元个数为1个；如果采用softmax分类器，输出层神经元个数为2个），而隐藏层层数以及隐藏层神经元是由人工设定。</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/1.jpg" class="lozad">

<h4 id="二-优化算法"><a href="#二-优化算法" class="headerlink" title="二. 优化算法"></a>二. 优化算法</h4><p>​    对于神经网络的优化算法，主要需要两步：<strong>前向传播(Forward Propagation)</strong>与<strong>反向传播(Back Propagation)</strong> </p>
<h5 id="1-前向传播算法"><a href="#1-前向传播算法" class="headerlink" title="1. 前向传播算法"></a>1. 前向传播算法</h5><p>​    <strong>前向传播</strong>就是<strong>从输入层到输出层，计算每一层每一个神经元的激活值</strong>。也就是<strong>先随机初始化每一层的参数矩阵，然后从输入层开始，依次计算下一层每个神经元的激活值，一直到最后计算输出层神经元的激活值</strong>。 </p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/2.jpg" class="lozad">

<p>（1）初始化参数矩阵$\space\Theta^{(1)}\space$与$\space\Theta^{(2)}\space$：<br>$$<br>\Theta^{(1)}=\begin{bmatrix}\theta_{10}^{(1)}&amp;\theta_{11}^{(1)}&amp;\theta_{12}^{(1)}&amp;\theta_{13}^{(1)}\\\theta_{20}^{(1)}&amp;\theta_{21}^{(1)}&amp;\theta_{22}^{(1)}&amp;\theta_{23}^{(1)}<br>\end{bmatrix}，\Theta^{(2)}=\begin{bmatrix}\theta_{10}^{(2)}&amp;\theta_{11}^{(2)}&amp;\theta_{12}^{(2)}\end{bmatrix}<br>$$<br>（2）计算隐藏层每个神经元的激活值值：<br>$$<br>a^{(2)}_1=g(\theta^{(1)}_{10}x_0+\theta^{(1)}_{11}x_1+\theta^{(1)}_{12}x_2+\theta^{(1)}_{13}x_3)\\a^{(2)}_2=g(\theta^{(1)}_{20}x_0+\theta^{(1)}_{21}x_1+\theta^{(1)}_{22}x_2+\theta^{(1)}_{23}x_3)<br>$$<br>即：<br>$$<br>a^{(2)}=g(\Theta^{(1)}x)，其中a^{(2)}=\begin{bmatrix}a^{(2)}_1\\a^{(2)}_2\end{bmatrix},x=\begin{bmatrix}x_0\\x_1\\x_2\\x_3\end{bmatrix}<br>$$<br>计算输出层的神经元的激活值：<br>$$<br>a^{(3)}_1=g(\theta^{(2)}_{10}a^{(2)}_0+\theta^{(2)}_{11}a^{(2)}_1+\theta^{(2)}_{12}a^{(2)}_2)<br>$$<br>即：<br>$$<br>a^{(3)}=g(\Theta^{(2)}a^{(2)})，其中a^{(2)}=\begin{bmatrix}a^{(2)}_0\\a^{(2)}_1\\a^{(2)}_2\end{bmatrix}<br>$$<br>其中$\space g(x)\space$为激活函数，常见的有$\space sigmoid\space$函数，$\space Tanh\space$函数，$\space ReLU\space$函数等。</p>
<h5 id="2-反向传播"><a href="#2-反向传播" class="headerlink" title="2. 反向传播"></a>2. 反向传播</h5><p>​    <strong>反向传播</strong>就是<strong>根据前向传播计算出来的激活值，来计算每一层参数的梯度，并从后往前进行参数的更新</strong>。</p>
<p>首先前向传播的函数嵌套关系如下：</p>
<ul>
<li>$a^{(1)}=x$</li>
<li>$z^{(2)}=\theta^{(1)}a^{(1)}$</li>
<li>$a^{(2)}=g(z^{(2)})$</li>
<li>$z^{(3)}=\theta^{(2)}a^{(2)}$</li>
<li>$h=a^{(3)}=g(z^{(3)})$</li>
</ul>
<p>$\Delta=\frac{\partial J(\theta)}{\partial\theta}$</p>
<p>第二层的代价函数表达式：<br>$$<br>\begin{align}<br>J(\theta)&amp;=-\frac{1}{m}\sum^m_{i=1}\left[y^{(i)}log\left(h_{\theta}(x^{(i)})\right)+(1-y^i)log\left(1-h_{\theta}(x^{(i)})\right)\right]\\<br>&amp;=-\frac{1}{m}\sum^m_{i=1}\left[y^ilog\left(g(\theta^{(2)}_ia^{(2)}_i\right)+(1-y^i)log\left(1-g(\theta^{(2)}_ia^{(2)}_i)\right)\right]<br>\end{align}<br>$$</p>
<p>$$<br>\frac{\partial J(\theta)}{\partial\theta}=-\frac{1}{m}\sum^m_{i=1}\left[y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}h^{‘}_{\theta}(x^{(i)})+(1-y^i)\frac{1}{1-h_{\theta}(x^{(i)})}(1-h_{\theta}(x))’\right]<br>$$</p>
<p>其中，<br>$$<br>\begin{align}<br>h_{\theta}^{‘}(x)&amp;=\left(\frac{1}{1+e^{-\theta^Tx}}\right)’\\<br>&amp;=\frac{e^{-\theta^Tx}}{(1+e^{-\theta^Tx})^2}(\theta^Tx)’<br>\end{align}<br>$$<br>代入，<br>$$<br>\begin{align}<br>\frac{\partial J(\theta)}{\partial\theta}&amp;=-\frac{1}{m}\sum^m_{i=1}\left[y^{(i)}\left(1+e^{-\theta^Tx^i}\right)\frac{e^{-\theta^Tx^i}}{(1+e^{-\theta^Tx^i})^2}(\theta^Tx^i)’-(1-y^i)\frac{1+e^{-\theta^Tx^i}}{e^{-\theta^Tx^i}}\frac{e^{-\theta^Tx^i}}{(1+e^{-\theta^Tx^i})^2}(\theta^Tx^i)’\right]\\<br>&amp;=-\frac{1}{m}\sum^m_{i=1}\left[y^{(i)}\frac{e^{-\theta^Tx^i}}{1+e^{-\theta^Tx^i}}(\theta^Tx^i)’-(1-y^i)\frac{1}{1+e^{-\theta^Tx^i}}(\theta^Tx^i)’\right]\\<br>&amp;=-\frac{1}{m}\sum^m_{i=1}\left[\left[y^{(i)}\left(1-\frac{1}{1+e^{-\theta^Tx^i}}\right)-(1-y^i)\frac{1}{1+e^{-\theta^Tx^i}}\right](\theta^Tx^i)‘\right]\\<br>&amp;=-\frac{1}{m}\sum^m_{i=1}\left[\left[y^{(i)}\left(1-h_{\theta}(x^{(i)})\right)-(1-y^i)h_{\theta}(x^{(i)})\right](\theta^Tx^i)‘\right]\\<br>&amp;=-\frac{1}{m}\sum^m_{i=1}\left[\left(y^{(i)}-h_{\theta}(x^{(i)})\right)(\theta^Tx^i)’\right]\\<br>&amp;=\frac{1}{m}\sum^m_{i=1}\left[\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x^i\right]<br>\end{align}<br>$$<br>所以第二层损失梯度可表示为：<br>$$<br>\begin{align}<br>\frac{\partial J(\theta)}{\partial\theta}&amp;=\frac{1}{m}\sum^m_{i=1}\left(g(\theta^{(2)}_ia^{(2)}_i)-y^i\right)a^{(2)}_i\\<br>&amp;=\frac{1}{m}\sum^m_{i=1}\left(a^{(3)}_i-y^i\right)a^{(2)}_i\\<br>&amp;=\frac{1}{m}\sum^m_{i=1}\delta^{(3)}_ia^{(2)}_i<br>\end{align}<br>$$<br>而对于单个参数<br>$$<br>\Delta^{(2)}=\frac{\partial J(\theta)}{\partial\theta^{(2)}}=\frac{\partial J(\theta)}{\partial a^{(3)}}\frac{\partial a^{(3)}}{\partial z^{(3)}}\frac{\partial z^{(3)}}{\partial\theta^{(2)}}=(h-y)a^{(2)}<br>$$<br>所以$\space\delta^{(3)}=h-y\space$</p>
<p>同理，第一层的损失梯度为：<br>$$<br>\Delta^{(1)}=\frac{\partial J(\theta)}{\partial\theta^{(2)}}=\frac{\partial J(\theta)}{\partial a^{(3)}}\frac{\partial a^{(3)}}{\partial z^{(3)}}\frac{\partial z^{(3)}}{\partial a^{(2)}}\frac{\partial a^{(2)}}{\partial z^{(2)}}\frac{\partial z^{(2)}}{\partial\theta^{(1)}}=\delta^{(3)}\theta^{(2)}g’(z^{(2)})a^{(1)}=\delta^{(2)}a^{(1)}<br>$$<br>其中，<br>$$<br>\delta^{(2)}=\delta^{(3)}\theta^{(2)}g’(z^{(2)})<br>$$</p>
<p>综上求得的梯度可以用到梯度下降来学习权重，并且前一层的权重可以由后一层推出。</p>
<h4 id="三-代码实现"><a href="#三-代码实现" class="headerlink" title="三. 代码实现"></a>三. 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="comment"># 网络层数</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        <span class="comment"># 网络的size</span></span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        <span class="comment"># 初始化参数b</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        <span class="comment"># 初始化权重w</span></span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line">    <span class="comment"># 批量随机梯度下降</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="comment"># 如果有测试集，则获取测试集的样本数量</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        <span class="comment"># 获得训练集的样本数量</span></span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="comment"># 损失</span></span><br><span class="line">        J_history = np.zeros((epochs, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(epochs):</span><br><span class="line">            <span class="comment"># 打乱训练集</span></span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            <span class="comment"># 构造小批量</span></span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="comment"># 循环训练mini_batch</span></span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            J_history[j] = self.compute_loss(test_data)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125; complete"</span>.format(j))</span><br><span class="line">        <span class="keyword">return</span> J_history</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># 反向</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line">    <span class="comment"># 计算预测正确的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line">    <span class="comment"># 损失</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        res = [(np.argmax(self.feedforward(x)),y)</span><br><span class="line">               <span class="keyword">for</span> (x,y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="comment"># print(data)</span></span><br><span class="line">        <span class="comment"># print(res)</span></span><br><span class="line">        loss = sum(np.power(x-y,<span class="number">2</span>) <span class="keyword">for</span> (x,y) <span class="keyword">in</span> res)/len(data)</span><br><span class="line">        <span class="comment"># print('loss epoch: ', loss)</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line"><span class="comment">#训练集，验证集，测试集分别为50000，10000，10000</span></span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br><span class="line"><span class="comment"># 构造神经网络</span></span><br><span class="line">net = Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># 批量随机梯度下降</span></span><br><span class="line">net.SGD(list(training_data),<span class="number">30</span>,<span class="number">4</span>,<span class="number">3</span>,list(test_data))</span><br><span class="line"><span class="comment"># with open('loss.txt', mode='a') as f:</span></span><br><span class="line"><span class="comment">#     f.write(loss)</span></span><br></pre></td></tr></table></figure>

<h5 id="1-隐藏层个数为10"><a href="#1-隐藏层个数为10" class="headerlink" title="1. 隐藏层个数为10"></a>1. 隐藏层个数为10</h5><p>结果：</p>
<blockquote>
<p>epoch 0，loss 2.1542</p>
<p>epoch 1，loss 1.9329</p>
<p>…</p>
<p>epoch 28，loss 1.5800</p>
<p>epoch 29，loss 1.5857</p>
<p>最终准确率为91.23%</p>
</blockquote>
<p>loss曲线</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/4.png" class="lozad">

<p>可以看到抖动的比较厉害，并且没有收敛。</p>
<h5 id="2-隐藏层个数为30"><a href="#2-隐藏层个数为30" class="headerlink" title="2. 隐藏层个数为30"></a>2. 隐藏层个数为30</h5><blockquote>
<p>epoch 0，loss 1.6010</p>
<p>epoch 1，loss 1.5101</p>
<p>…</p>
<p>epoch 28，loss 0.8978</p>
<p>epoch 29，loss 0.8789</p>
<p>最终准确率为95.34%</p>
</blockquote>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/3.png" class="lozad">

<p>可以看到loss下降的更多，更接近收敛了，并且最终预测的准确率也高了不少</p>
<h5 id="3-隐藏层个数为100"><a href="#3-隐藏层个数为100" class="headerlink" title="3. 隐藏层个数为100"></a>3. 隐藏层个数为100</h5><blockquote>
<p>epoch 0，loss 1.5833</p>
<p>epoch 1，loss 1.0227</p>
<p>…</p>
<p>epoch 28，loss 0.7085</p>
<p>epoch 29，loss 0.6705</p>
<p>最终准确率为96.57%</p>
</blockquote>
<p>loss曲线：</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/5.png" class="lozad">

<h5 id="4-隐藏层个数为300"><a href="#4-隐藏层个数为300" class="headerlink" title="4. 隐藏层个数为300"></a>4. 隐藏层个数为300</h5><blockquote>
<p>epoch 0，loss 12.8265</p>
<p>epoch 1，loss 9.6892</p>
<p>…</p>
<p>epoch 28，loss 7.8339</p>
<p>epoch 29，loss 7.4132</p>
<p>最终准确率为57.89%</p>
</blockquote>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/6.png" class="lozad">

<p>最后一个1000个隐藏层的由于机器原因，实在跑不下来。综上可知，隐藏层个数适中（在上面几个实验100是最好的）最好，太小了准确率会比较低，太大了loss收敛得更慢，需要更多的epoch来收敛。</p>
<p>所以对应的不同epoch，epoch越大，即训练迭代次数越多，loss更趋向于收敛，但可能导致在训练集上过拟合问题。前面3个实验30个epoch已经足以使loss收敛了，而最后一个实验明显需要更大的迭代次数。</p>
<p>前面几组实验学习率为3，接下来改变学习率，隐藏层个数固定为10。</p>
<h5 id="1-学习率为0-3"><a href="#1-学习率为0-3" class="headerlink" title="1. 学习率为0.3"></a>1. 学习率为0.3</h5><blockquote>
<p>epoch 0，loss 4.4042</p>
<p>epoch 1，loss 2.8017</p>
<p>…</p>
<p>epoch 28，loss 1.5790</p>
<p>epoch 29，loss 1.5553</p>
<p>最终准确率为90.33%</p>
</blockquote>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/7.png" class="lozad">

<p>可以看到loss曲线比之前的更平滑。</p>
<h5 id="2-学习率为0-03"><a href="#2-学习率为0-03" class="headerlink" title="2. 学习率为0.03"></a>2. 学习率为0.03</h5><blockquote>
<p>epoch 0，loss 13.4632</p>
<p>epoch 1，loss </p>
<p>…</p>
<p>epoch 28，loss 2.6922</p>
<p>epoch 29，loss 2.6539</p>
<p>最终准确率为 83.73%</p>
</blockquote>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/8.png" class="lozad">

<p>综上，可以发现学习率降低，对应的loss曲线会收敛得更加平滑，但同时，收敛速度也会降低。</p>
<p>正则化可以保留所有的特征变量，但是会减小特征变量的数量级，可以避免数据训练过拟合问题，同时加快了训练的速度。</p>
<h4 id="四-Hebb学习规则"><a href="#四-Hebb学习规则" class="headerlink" title="四. Hebb学习规则"></a>四. Hebb学习规则</h4><p>唐纳德·赫布（1904-1985）是加拿大著名生理心理学家。Hebb学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。<br> 巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。<br> 受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。</p>
<p> Hebb学习律可表示为：<br>$$<br>W_{ij}(t+1)=W_{ij}(t)+a\cdot y_i\cdot y_j<br>$$<br>其中$\space W_{ij}\space$表示神经元$\space j\space$到神经元$\space i\space$的连接权，$\space y_i\space$与$\space y_j\space$表示两个神经元的输出，$\space a\space$是表示学习速率的常数，如果$\space y_i\space$与$\space y_j \space$同时被激活，即$\space y_i\space$与$\space y_j\space$同时为正，那么$\space w_{ij}\space$将增大。如果$\space y_i\space$被激活，而$\space y_j\space$处于抑制状态，即$\space y_i\space$为正$\space y_j\space$为负，那么$\space w_{ij}\space$将变小。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/01/15/ml-4/">http://yoursite.com/2020/01/15/ml-4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">阿瑜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a><a class="post-meta__tags" href="/tags/神经网络/">神经网络    </a><a class="post-meta__tags" href="/tags/Hebb学习规则/">Hebb学习规则    </a><a class="post-meta__tags" href="/tags/前向传播/">前向传播    </a><a class="post-meta__tags" href="/tags/反向传播/">反向传播    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/4/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.png"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.png"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/15/data-science-4/"><img class="prev_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/4/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>梯度下降法</span></div></a></div><div class="next-post pull-right"><a href="/2020/01/15/data-science-3/"><img class="next_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/3/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>线性回归和逻辑回归</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/01/15/ml-5/" title="softmax多分类"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/5/cover.jpg"><div class="relatedPosts_title">softmax多分类</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-5/" title="商品价格预测"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/5/cover.jpg"><div class="relatedPosts_title">商品价格预测</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/ml-1/" title="MNIST数据集处理"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/1/cover.jpg"><div class="relatedPosts_title">MNIST数据集处理</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-3/" title="线性回归和逻辑回归"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/3/cover.jpg"><div class="relatedPosts_title">线性回归和逻辑回归</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-4/" title="梯度下降法"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/DataScience/4/cover.jpg"><div class="relatedPosts_title">梯度下降法</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/ml-3/" title="FINDS算法及决策树ID3算法实现"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@4.0/MachineLearning/3/cover.jpg"><div class="relatedPosts_title">FINDS算法及决策树ID3算法实现</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div id="footer"><div class="copyright">&copy;2018 - 2020 By Yu</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Welcome to my <a href="https://darkseidddddd.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></section><div class=" " id="post_bottom"><div id="post_bottom_items"><a id="mobile_to_comment" href="#post-comment"><i class="mobile_scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#一-神经网络"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一. 神经网络</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#二-优化算法"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">二. 优化算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-前向传播算法"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">1. 前向传播算法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-反向传播"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">2. 反向传播</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#三-代码实现"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">三. 代码实现</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-隐藏层个数为10"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">1. 隐藏层个数为10</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-隐藏层个数为30"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">2. 隐藏层个数为30</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-隐藏层个数为100"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">3. 隐藏层个数为100</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#4-隐藏层个数为300"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">4. 隐藏层个数为300</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-学习率为0-3"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">1. 学习率为0.3</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-学习率为0-03"><span class="toc_mobile_items-number">3.6.</span> <span class="toc_mobile_items-text">2. 学习率为0.03</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#四-Hebb学习规则"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">四. Hebb学习规则</span></a></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>