<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>商品价格预测 | 阿瑜</title><meta name="description" content="商品价格预测"><meta name="keywords" content="数据科学,数据分析,机器学习,回归,数据挖掘,神经网络"><meta name="author" content="Yu"><meta name="copyright" content="Yu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon1.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2020/01/15/data-science-5/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="商品价格预测"><meta name="twitter:description" content="商品价格预测"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="商品价格预测"><meta property="og:url" content="http://yoursite.com/2020/01/15/data-science-5/"><meta property="og:site_name" content="阿瑜"><meta property="og:description" content="商品价格预测"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="线性回归和逻辑回归" href="http://yoursite.com/2020/01/15/data-science-3/"><link rel="next" title="Hello World" href="http://yoursite.com/2019/09/20/hello-world/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'false',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#背景："><span class="toc-number">1.</span> <span class="toc-text">背景：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分析目标："><span class="toc-number">2.</span> <span class="toc-text">分析目标：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据字段分析"><span class="toc-number">3.</span> <span class="toc-text">数据字段分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据集"><span class="toc-number">4.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#评价指标"><span class="toc-number">5.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#一-数据集"><span class="toc-number">6.</span> <span class="toc-text">一. 数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#二-数据分析及预处理"><span class="toc-number">7.</span> <span class="toc-text">二. 数据分析及预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#三-特征选择"><span class="toc-number">8.</span> <span class="toc-text">三. 特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#四-模型构建"><span class="toc-number">9.</span> <span class="toc-text">四. 模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-岭回归模型"><span class="toc-number">9.1.</span> <span class="toc-text">1. 岭回归模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-神经网络模型"><span class="toc-number">9.2.</span> <span class="toc-text">2. 神经网络模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-LightGBM"><span class="toc-number">9.3.</span> <span class="toc-text">3. LightGBM</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#五-预测及结果"><span class="toc-number">10.</span> <span class="toc-text">五. 预测及结果</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-岭回归模型-1"><span class="toc-number">10.1.</span> <span class="toc-text">1. 岭回归模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-神经网络模型-1"><span class="toc-number">10.2.</span> <span class="toc-text">2. 神经网络模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-lightGBM"><span class="toc-number">10.3.</span> <span class="toc-text">3. lightGBM</span></a></li></ol></li></ol><li class="toc-item toc-level-3"><a class="toc-link" href="#六-遇到的问题"><span class="toc-number"></span> <span class="toc-text">六. 遇到的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-清洗item-description"><span class="toc-number">0.1.</span> <span class="toc-text">1. 清洗item_description</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-清洗奇异值"><span class="toc-number">0.2.</span> <span class="toc-text">2. 清洗奇异值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Tfidf与Count"><span class="toc-number">0.3.</span> <span class="toc-text">3. Tfidf与Count</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#七-调用的库"><span class="toc-number"></span> <span class="toc-text">七. 调用的库</span></a></li></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/cover.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">阿瑜</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><div class="menu_mask"></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@2.0/cover/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item text-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">5</div></a></div></div><div class="mobile_data_item text-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item text-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-igloo"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-window-restore"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-stream"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-robot"></i><span> About</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">商品价格预测</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-01-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-02-05</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/数据科学导论/">数据科学导论</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h4 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h4><ul>
<li>考虑到网上销售的产品数量，产品定价在规模上变得更加困难。服装有很强的季节性定价趋势，受品牌影响很大，而电子产品价格根据产品规格波动。如何根据以往信息合理定价，有效地帮助商家进行商品的销售是一个十分有意义的问题。</li>
</ul>
<h4 id="分析目标："><a href="#分析目标：" class="headerlink" title="分析目标："></a>分析目标：</h4><p>通过给出的商品描述、商品类别和品牌信息，并结合训练数据种的商品价格来给新商品顶价格。Eg：</p>
<table>
<thead>
<tr>
<th align="center">商品名称</th>
<th align="center">品牌名称</th>
<th align="center">商品描述</th>
<th align="center">商品类别</th>
</tr>
</thead>
<tbody><tr>
<td align="center">美杜莎羊皮飞行员夹克外套男</td>
<td align="center">Versace</td>
<td align="center">1、时尚衣领设计，经典百搭，休闲舒适；2、简约袖口设计；细节尽显品质；3、精湛的制作工艺，细节彰显品质；4、精心挑选高品质面料，手感好，面料舒适</td>
<td align="center">服饰</td>
</tr>
<tr>
<td align="center">新款秋冬季男士韩版潮流连帽帅气夹克衣服</td>
<td align="center">美特斯邦威</td>
<td align="center">一件精心设计、不挑身材的保暖夹克。舒适，时尚，有型。</td>
<td align="center">服饰</td>
</tr>
</tbody></table>
<p>显然Versace的衣服价格上应该远高于美特斯邦威的衣服，并且在商品描述中，可以发现两者描述有细微差别。</p>
<blockquote>
<p> 本project旨在对文本信息进行分析，提取文本信息中重要的信息，推导出和价格之间的潜在关系</p>
</blockquote>
<h4 id="数据字段分析"><a href="#数据字段分析" class="headerlink" title="数据字段分析"></a>数据字段分析</h4><table>
<thead>
<tr>
<th align="center">字段名</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">train_id/test_id</td>
<td align="center">商品的序号值</td>
</tr>
<tr>
<td align="center">name</td>
<td align="center">商品名称</td>
</tr>
<tr>
<td align="center">category_name</td>
<td align="center">商品所属类别</td>
</tr>
<tr>
<td align="center">item_condition_id</td>
<td align="center">商品当前是否有货</td>
</tr>
<tr>
<td align="center">brand_name</td>
<td align="center">商品品牌</td>
</tr>
<tr>
<td align="center">shipping</td>
<td align="center">是否包邮</td>
</tr>
<tr>
<td align="center">item_description</td>
<td align="center">商品描述</td>
</tr>
<tr>
<td align="center">price</td>
<td align="center">商品</td>
</tr>
</tbody></table>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li>train.csv 训练集（含price）</li>
<li>test.csv 测试集（不含price）</li>
<li>label_test.csv 测试集中对应的price</li>
<li>f_test.csv 最终的评价数据集（不含price）</li>
</ul>
<h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><p>评价使用的是 Mean Squared Logarithmic Error: 计算方式如下<br>$$<br>MSLE=\frac{1}{n}\sum^n_{i=1}(log(p_i+1)-log(\alpha_i+1))^2<br>$$<br>其中 $n$ 代表测试集的样本数；$p_i$ 代表的是预测的商品的价格值；$\alpha_i$ 代表实际的销售价格</p>
<hr>
<hr>
<h4 id="一-数据集"><a href="#一-数据集" class="headerlink" title="一. 数据集"></a>一. 数据集</h4><table>
<thead>
<tr>
<th align="center">字段名</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">name</td>
<td align="center">商品名称</td>
</tr>
<tr>
<td align="center">category_name</td>
<td align="center">商品所属类别</td>
</tr>
<tr>
<td align="center">item_condition_id</td>
<td align="center">商品当前是否有货</td>
</tr>
<tr>
<td align="center">brand_name</td>
<td align="center">商品品牌</td>
</tr>
<tr>
<td align="center">shipping</td>
<td align="center">是否包邮</td>
</tr>
<tr>
<td align="center">item_description</td>
<td align="center">商品描述</td>
</tr>
<tr>
<td align="center">price</td>
<td align="center">商品价格</td>
</tr>
</tbody></table>
<p>训练集：train.csv</p>
<p>最终要测试的数据：test.csv</p>
<p>这里我们用留出法，将train.csv分成训练集 : 测试集 = 7 : 3</p>
<h4 id="二-数据分析及预处理"><a href="#二-数据分析及预处理" class="headerlink" title="二. 数据分析及预处理"></a>二. 数据分析及预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'data/train.csv'</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line"><span class="comment"># 查看数据的统计信息</span></span><br><span class="line">data.describe()</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/12.png" class="lozad">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看缺失值</span></span><br><span class="line">data.isnull().any()</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/10.png" class="lozad">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.isnull().sum()</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/11.png" class="lozad">

<p>可以看到其中<strong>category_name</strong>和<strong>brand_name</strong>这两个特征存在缺失值，并且<strong>brand_name</strong>缺失值的数量比较多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">featureProcessing</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># 删除不用的数据</span></span><br><span class="line">    df = df.drop([<span class="string">'price'</span>, <span class="string">'train_id'</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 处理缺失值</span></span><br><span class="line">    df[<span class="string">'category_name'</span>] = df[<span class="string">'category_name'</span>].fillna(<span class="string">'No/No/No'</span>).astype(str)</span><br><span class="line">    df[<span class="string">'brand_name'</span>] = df[<span class="string">'brand_name'</span>].fillna(<span class="string">'missing'</span>).astype(str)</span><br><span class="line">    df[<span class="string">'item_description'</span>] = df[<span class="string">'item_description'</span>].fillna(<span class="string">'No'</span>)</span><br><span class="line">    <span class="comment"># 将int型转为str类型</span></span><br><span class="line">    df[<span class="string">'shipping'</span>] = df[<span class="string">'shipping'</span>].astype(str)</span><br><span class="line">    df[<span class="string">'item_condition_id'</span>] = df[<span class="string">'item_condition_id'</span>].astype(str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line">df = featureProcessing(data)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/13.png" class="lozad"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制price柱形图</span></span><br><span class="line">width = [<span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>, <span class="number">200</span>, <span class="number">250</span>, <span class="number">300</span>, <span class="number">350</span>]</span><br><span class="line">sns.distplot(train_data[<span class="string">'price'</span>], color = <span class="string">'g'</span>, rug = <span class="literal">True</span>, bins = [w <span class="keyword">for</span> w <span class="keyword">in</span> width])</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/15.png" class="lozad">

<p>可以看到<strong>price</strong>的频率分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用原始数据的int类型画图</span></span><br><span class="line"><span class="comment"># 统计item_condition不同量出现的次数</span></span><br><span class="line">ax = sns.countplot(<span class="string">'item_condition_id'</span>, data = df)</span><br><span class="line">ax.set_xticklabels(ax.get_xticklabels(), rotation = <span class="number">40</span>, ha = <span class="string">"right"</span>)</span><br><span class="line">ax.set_title(<span class="string">'Count of each item condition'</span>)</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/14.png" class="lozad">

<p>可以看到为取值为1的分布最多，取值为4，5的相对较少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 经过np.log压缩后绘制price柱形图</span></span><br><span class="line">plt.xlabel(<span class="string">'log(price+1)'</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">'Log Price Distribution'</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">np.log(train_data[<span class="string">'price'</span>] + <span class="number">1</span>).plot.hist(bins = <span class="number">50</span>, figsize = (<span class="number">8</span>, <span class="number">4</span>), edgecolor = <span class="string">'white'</span>, color = <span class="string">'g'</span>)</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/16.png" class="lozad">

<p>经过log处理后的价格分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出列类型</span></span><br><span class="line">print(df.astype(<span class="string">'object'</span>).describe().transpose())</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/17.png" class="lozad">

<p>可以看到每个特征的具体信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_cat</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> text.split(<span class="string">"/"</span>)</span><br><span class="line">df[<span class="string">'cat1'</span>], df[<span class="string">'cat2'</span>], df[<span class="string">'cat3'</span>] = \</span><br><span class="line">zip(*df[<span class="string">'category_name'</span>].apply(<span class="keyword">lambda</span> x: split_cat(x)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看切分后的cat1的取值的概率</span></span><br><span class="line">print(df[<span class="string">'cat1'</span>].value_counts() / len(df[<span class="string">'cat1'</span>]))</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/18.png" class="lozad">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Category切分之后的第一部分的出现次数最多的前10个</span></span><br><span class="line">plt.figure(figsize = (<span class="number">17</span>, <span class="number">10</span>))</span><br><span class="line">sns.countplot(y = df[<span class="string">'cat1'</span>], order = df[<span class="string">'cat1'</span>].value_counts().index, orient = <span class="string">'v'</span>)</span><br><span class="line">plt.title(<span class="string">'Top 10 Cat1 Categories'</span>, fontsize = <span class="number">25</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cat1 Category'</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Number of Items'</span>)</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/22.png" class="lozad"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过cat1绘制二元特征shipping的柱形分布图</span></span><br><span class="line">ax = sns.countplot(x = <span class="string">"cat1"</span>, hue = <span class="string">"shipping"</span>, data = df, palette = <span class="string">"Set3"</span>)</span><br><span class="line">ax.set_xticklabels(ax.get_xticklabels(), rotation = <span class="number">40</span>, ha = <span class="string">"right"</span>)</span><br><span class="line">ax.set_title(<span class="string">'Count of shipping by cat1'</span>)</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/19.png" class="lozad">

<p>挑选<strong>cat1</strong>和<strong>shipping</strong>来看一看分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过shipping绘制price的柱形分布图，查看shipping和price的相关性</span></span><br><span class="line">shipping_by_buyer = data[data[<span class="string">'shipping'</span>] == <span class="number">0</span>][<span class="string">'price'</span>]</span><br><span class="line">shipping_by_seller = data[data[<span class="string">'shipping'</span>] == <span class="number">1</span>][<span class="string">'price'</span>]</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">18</span>, <span class="number">8</span>))</span><br><span class="line">ax.hist(shipping_by_seller, color = <span class="string">'g'</span>, alpha = <span class="number">1.0</span>, bins = <span class="number">50</span>, range = [<span class="number">0</span>, <span class="number">100</span>],label = <span class="string">'Price when Seller pays Shipping'</span>)</span><br><span class="line">ax.hist(shipping_by_buyer, color = <span class="string">'y'</span>, alpha = <span class="number">0.7</span>, bins = <span class="number">50</span>, range = [<span class="number">0</span>, <span class="number">100</span>],label = <span class="string">'Price when Buyer pays Shipping'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'price'</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">plt.ylabel(<span class="string">'frequency'</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">'Price Distribution by Shipping Type'</span>, fontsize = <span class="number">15</span>)</span><br><span class="line">plt.tick_params(labelsize = <span class="number">12</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/20.png" class="lozad">

<p>可以看到包邮时整体平均价格会相对低一些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Brand按照价格排序的前20个</span></span><br><span class="line">top_20_exp_brand = pd.DataFrame(train_data.groupby([<span class="string">'brand_name'</span>],as_index = <span class="literal">True</span>).std()[<span class="string">'price'</span>].sort_values(ascending = <span class="literal">False</span>)[<span class="number">0</span> : <span class="number">20</span>]).reset_index()</span><br><span class="line">ax = sns.barplot(x = <span class="string">"brand_name"</span>, y = <span class="string">"price"</span>, data = top_20_exp_brand)</span><br><span class="line">ax.set_xticklabels(ax.get_xticklabels(),rotation = <span class="number">90</span>)</span><br><span class="line">ax.set_title(<span class="string">'Top 20 Brand'</span>, fontsize = <span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/21.png" class="lozad">



<p>可以看到商标对价格也有很大的影响。</p>
<h4 id="三-特征选择"><a href="#三-特征选择" class="headerlink" title="三. 特征选择"></a>三. 特征选择</h4><p>考虑到最初的六个特征并不能很好地表达模型以及做出很好的预测，所以我们调试了无数次。最终得到如下的特征组合。</p>
<ul>
<li><strong>name</strong>和<strong>item_description</strong></li>
</ul>
<p>对于能很好的代表一个商品的特征，我们初步分析有<strong>name</strong>和<strong>item_description</strong>这两个特征，所以我们分别去掉这两个特征，在岭回归模型上，得到了一个更加糟糕的结果。于是我们得出一个结论——这两个特征很有用。</p>
<ul>
<li><strong>brand_name</strong>和<strong>name</strong></li>
</ul>
<p>对于单独的商标，例如Nike，能表达的商品信息比较宽泛，价格也参差不齐。同理，对于单独的商品名称，例如××T-shirt Medium，也是如此。但是如果将商标和名称合在一起，可能就会更好地表达出这件商品的特征，例如Nike black T-shirt Medium，它将价格区间变得更小，即更具体，这可能就是我们需要的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'brand_name_name'</span>] = df[<span class="string">'brand_name'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'name'</span>].fillna(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/2.png" class="lozad"></p>
<ul>
<li><strong>name</strong>和<strong>cat2</strong></li>
</ul>
<p>首先我们先将<strong>category_name</strong>按照’/‘分隔切开，新生成三列类别特征，分别是<strong>cat1，cat2，cat3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_cat</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> text.split(<span class="string">"/"</span>)</span><br><span class="line">df[<span class="string">'cat1'</span>], df[<span class="string">'cat2'</span>], df[<span class="string">'cat3'</span>] = \</span><br><span class="line">zip(*df[<span class="string">'category_name'</span>].apply(<span class="keyword">lambda</span> x: split_cat(x)))</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/9.png" class="lozad"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'cat1: %d, cat2: %d, cat3: %d'</span> %(cat1.nunique(), cat2.nunique(), cat3.nunique()))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>cat1: 11, cat2: 114, cat3: 816</p>
</blockquote>
<p>其中<strong>cat1</strong>有11个不同的类，<strong>cat2</strong>有114个不同的类，<strong>cat3</strong>有816个不同的类。</p>
<p>由之前的分析，我们认为name和item_description非常重要，而要能表示一兼商品的价格，商品类别也很重要，所以在这个前提之下，我们分别融合了name和cat1，cat2，cat3，进行了试探，最终发现name和cat2的组合（或者name和cat3）的组合，使得最终结果能更加优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name_cat1'</span>] = df[<span class="string">'name'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'cat1'</span>].fillna(<span class="string">''</span>)</span><br><span class="line">df[<span class="string">'name_cat2'</span>] = df[<span class="string">'name'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'cat2'</span>].fillna(<span class="string">''</span>)</span><br><span class="line">df[<span class="string">'name_cat3'</span>] = df[<span class="string">'name'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'cat3'</span>].fillna(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop([<span class="string">'name_cat1'</span>, <span class="string">'name_cat3'</span>])</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/4.png" class="lozad"></p>
<ul>
<li>name，item_description和brand_name</li>
</ul>
<p>对于上面已经用过的特征，不妨把item_description和name结合在一起，同时再加一个brand_name，这样获得的特征，又进一步提升了最后的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'text'</span>] = df[<span class="string">'item_description'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'name'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + df[<span class="string">'brand_name'</span>].fillna(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/5.png" class="lozad"></p>
<ul>
<li>brand_name，cat23，id和shipping</li>
</ul>
<p>所有特征中，表达能力最弱的就是<strong>brand_name</strong>，<strong>condition_id</strong>和<strong>shipping</strong>了，但是如果将它们拼接在一起，就会使它们的表达能力增强，同时，我们还加入了类别，总共重组了四个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'bn_cat23_id_shipping'</span>] = df[<span class="string">'brand_name'</span>] + df[<span class="string">'cat23'</span>]+df[<span class="string">'item_condition_id'</span>]+df[<span class="string">'shipping'</span>]</span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/6.png" class="lozad"></p>
<p>于是，我们得到了我们想要的重组之后的特征组合。为了降低特征维度，我们试图寻找可以删减不用的特征。因为<strong>text</strong>这个特征在合并的时候，之间加了分隔符空格，再加上TfidfVectorizer会使得整个句子按照默认字符空格将所有单词分开，所以<strong>text</strong>实际上包含了<strong>name</strong>和<strong>item_description</strong>这两个特征。经过验证，在删掉这两个特征时，最终结果并没有太大的变化，但对于其它特征来说，删去则会使结果表现得更糟糕。最终，我们便确定了我们所需要的特征组合。</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/7.png" class="lozad"></p>
<h4 id="四-模型构建"><a href="#四-模型构建" class="headerlink" title="四. 模型构建"></a>四. 模型构建</h4><h5 id="1-岭回归模型"><a href="#1-岭回归模型" class="headerlink" title="1. 岭回归模型"></a>1. 岭回归模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeClassify</span><span class="params">(train_data, train_label)</span>:</span></span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">'auto'</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=<span class="number">3.5</span>,</span><br><span class="line">        max_iter=<span class="number">550</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br></pre></td></tr></table></figure>

<h5 id="2-神经网络模型"><a href="#2-神经网络模型" class="headerlink" title="2. 神经网络模型"></a>2. 神经网络模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regressor_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># 第一层，64个节点，随机初始化权重，激活函数为relu，输入维度为特征数量</span></span><br><span class="line">    model.add(Dense(units = <span class="number">64</span>,kernel_initializer = <span class="string">'uniform'</span>, activation = <span class="string">'relu'</span>, input_dim = <span class="number">449780</span>))</span><br><span class="line">    <span class="comment"># 第二层，16个节点，激活函数为relu</span></span><br><span class="line">    model.add(Dense(units = <span class="number">16</span>, kernel_initializer = <span class="string">'uniform'</span>, activation = <span class="string">'relu'</span>))</span><br><span class="line">    <span class="comment"># 第三层，16个节点，激活函数为relu</span></span><br><span class="line">    model.add(Dense(units = <span class="number">16</span>, kernel_initializer = <span class="string">'uniform'</span>, activation = <span class="string">'relu'</span>))</span><br><span class="line">    <span class="comment"># 第四层，8个节点，激活函数为relu</span></span><br><span class="line">    model.add(Dense(units = <span class="number">8</span>, kernel_initializer = <span class="string">'uniform'</span>, activation = <span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, init=<span class="string">'uniform'</span>))</span><br><span class="line">    <span class="comment"># 编译模型，因为y_train已经log过，所以这里用mse评估函数，相当于最终的msle</span></span><br><span class="line">    model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'mse'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">estimators = []</span><br><span class="line"><span class="comment"># 设置验证集，并在训练时输出</span></span><br><span class="line">estimators.append((<span class="string">'mlp'</span>, KerasRegressor(build_fn= regressor_model, epochs=<span class="number">10</span>, validation_data=(X_test, y_test),verbose=<span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 添加到pipeline</span></span><br><span class="line">pipeline = Pipeline(estimators)</span><br><span class="line"></span><br><span class="line">pipeline.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>

<h5 id="3-LightGBM"><a href="#3-LightGBM" class="headerlink" title="3. LightGBM"></a>3. LightGBM</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'task'</span>: <span class="string">'train'</span>,</span><br><span class="line">    <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,  <span class="comment"># 设置提升类型为gbdt</span></span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'regression'</span>, <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="string">'metric'</span>: &#123;<span class="string">'l2'</span>,<span class="string">'mse'</span>&#125;,  <span class="comment"># 评估函数mse</span></span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">31</span>,   <span class="comment"># 叶子节点数</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.1</span>,  <span class="comment"># 学习速率</span></span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">1</span>, <span class="comment"># 建树的特征选择比例</span></span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.9</span>, <span class="comment"># 建树的样本采样比例</span></span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,  <span class="comment"># k 意味着每 k 次迭代执行bagging</span></span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">1</span>, <span class="comment"># 显示信息</span></span><br><span class="line">    <span class="string">'batch_size'</span>: <span class="number">48</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train) <span class="comment"># 将数据保存到LightGBM二进制文件将使加载更快</span></span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  <span class="comment"># 创建验证数据</span></span><br><span class="line">gbm = lgb.train(params,lgb_train,num_boost_round=<span class="number">3000</span>,valid_sets=lgb_eval,early_stopping_rounds=<span class="number">15</span>) <span class="comment"># 训练数据需要参数列，总共训练3000次，当连续15次不下降时退出训练</span></span><br></pre></td></tr></table></figure>

<h4 id="五-预测及结果"><a href="#五-预测及结果" class="headerlink" title="五. 预测及结果"></a>五. 预测及结果</h4><h5 id="1-岭回归模型-1"><a href="#1-岭回归模型-1" class="headerlink" title="1. 岭回归模型"></a>1. 岭回归模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeClassify</span><span class="params">(train_data, train_label, alpha)</span>:</span></span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">'auto'</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=alpha,</span><br><span class="line">        max_iter=<span class="number">550</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br><span class="line"></span><br><span class="line">res_alpha = []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">3</span>,<span class="number">3.1</span>,<span class="number">3.2</span>,<span class="number">3.3</span>,<span class="number">3.4</span>,<span class="number">3.5</span>,<span class="number">3.6</span>,<span class="number">3.7</span>,<span class="number">3.8</span>,<span class="number">3.9</span>,<span class="number">4</span>]:</span><br><span class="line">    ridgeClf = ridgeClassify(X_train, y_train, alpha)</span><br><span class="line"><span class="comment"># 结果预测</span></span><br><span class="line">    test_price = ridgeClf.predict(X_test)</span><br><span class="line">    msle = mean_squared_log_error(y_test, np.expm1(test_price))</span><br><span class="line">    res_alpha.append(msle)</span><br><span class="line">res_alpha</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">def ridgeClassify(train_data, train_label,max_iter):</span></span><br><span class="line"><span class="string">    ridgeClf = Ridge(</span></span><br><span class="line"><span class="string">        solver='auto',</span></span><br><span class="line"><span class="string">        fit_intercept=True,</span></span><br><span class="line"><span class="string">        alpha=3.5,</span></span><br><span class="line"><span class="string">        max_iter=max_iter,</span></span><br><span class="line"><span class="string">        normalize=False,</span></span><br><span class="line"><span class="string">        tol=0.01)</span></span><br><span class="line"><span class="string">    # 训练</span></span><br><span class="line"><span class="string">    ridgeClf.fit(train_data, train_label)</span></span><br><span class="line"><span class="string">    return ridgeClf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">res_iter = []</span></span><br><span class="line"><span class="string">for iters in [400,450,500,550,600,650,700]:</span></span><br><span class="line"><span class="string">    ridgeClf = ridgeClassify(X_train, y_train, iters)</span></span><br><span class="line"><span class="string"># 结果预测</span></span><br><span class="line"><span class="string">    test_price = ridgeClf.predict(X_test)</span></span><br><span class="line"><span class="string">    msle = mean_squared_log_error(y_test, np.expm1(test_price))</span></span><br><span class="line"><span class="string">    res_iter.append(msle)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/28.png" class="lozad"></p>
<p>这里是我们测试的不同学习率对结果的影响，可以看到当alpha为3.3，3.4，3.5的时候，结果最好。</p>
<p>为了寻找最优的参数组合，我们还分别测试了不同的最大迭代次数以及不同的精度对结果的影响。</p>
<p>开始我们分别测试了学习率为0.001，0.01，0.05，0.1，1，3，5，7的各种不同情况，发现当学习率为3的时候，结果是最理想的。在这基础之上，之后我们又分别测试了alpha为区间[3,4]，间隔为0.1的所有不同取值，最终得到结果，发现当alpha为3.5的时候，能使我们训练出一个相对最好的模型。</p>
<p>接着，我们保持alpha不变，分别测试了max_iter为400，450，500，550，600，650，700的各种不同取值，最终得到结果，max_iter在以上区间内的取值并不会对模型产生太大的影响，所以我们取550为最终的最优参数。</p>
<h5 id="2-神经网络模型-1"><a href="#2-神经网络模型-1" class="headerlink" title="2. 神经网络模型"></a>2. 神经网络模型</h5><p>最初我们只设置了两层，第一层的神经元个数为8，第二层的神经元个数为4，激活函数为relu，epochs为5，得到如下的结果：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/26.png" class="lozad"></p>
<p>可以看到由于模型太过于简单，最终的效果并不好。</p>
<p>于是我们将两层扩充至三层，第一层的神经元个数为16，第二层的神经元个数为8，第三层神经元个数为4，激活函数为relu，epochs为5，得到如下结果：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/30.png" class="lozad"></p>
<p>可以看到第二个epoch就已经到了最优的结果了，所以证明模型还不够复杂，于是加下来我们又更改了模型参数。</p>
<p>最后我们扩充至四层模型，第一层到第四层的神经元个数分别为64，16，16，8，激活函数为relu，得到如下的结果：</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/8.png" class="lozad">

<p>因为我们的特征维度太大，所以我们没有再复杂化整个模型，可以看到效果并没有岭回归好，并且训练速度也非常慢，在第一个epoch后就基本不会再提升了。最终我们放弃了这个模型。</p>
<h5 id="3-lightGBM"><a href="#3-lightGBM" class="headerlink" title="3. lightGBM"></a>3. lightGBM</h5><p>刚开始参数设置</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/24.png" class="lozad">

<p>总共迭代2000次，最终结果如下：</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/23.jpg" class="lozad">

<p>可以看到结果在0.2099的样子。</p>
<p>然后我们改变参数如下：</p>
<img style="zoom:80%;" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/25.png" class="lozad">

<p>即增大了叶子节点数，其它保持不变。</p>
<p>最终结果如下：</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/27.png" class="lozad"></p>
<p>可以看到相对于上面的结果，叶子节点数增加了一倍，而最终结果只降低了0.004。还是没有岭回归得到的效果好。所以我们放弃了这个模型。</p>
<p>最终，我们在岭回归上达到了0.197的msle，这也是我们在划分方式为7 : 3训练集上的最终结果。</p>
<h3 id="六-遇到的问题"><a href="#六-遇到的问题" class="headerlink" title="六. 遇到的问题"></a>六. 遇到的问题</h3><h5 id="1-清洗item-description"><a href="#1-清洗item-description" class="headerlink" title="1. 清洗item_description"></a>1. 清洗<strong>item_description</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> sent_tokenize</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line">stop = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment"># 将数字，换行，\t等全部替换为空格</span></span><br><span class="line">    regex = re.compile(<span class="string">'['</span> +re.escape(string.punctuation) + <span class="string">'0-9\\r\\t\\n]'</span>)</span><br><span class="line">    text = regex.sub(<span class="string">" "</span>, text)</span><br><span class="line">        </span><br><span class="line">    tokens_ = [word_tokenize(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent_tokenize(text)]</span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="keyword">for</span> token_by_sent <span class="keyword">in</span> tokens_:</span><br><span class="line">        tokens += token_by_sent</span><br><span class="line">    <span class="comment"># 全部单词小写并去掉停用词</span></span><br><span class="line">    tokens = list(filter(<span class="keyword">lambda</span> t: t.lower() <span class="keyword">not</span> <span class="keyword">in</span> stop, tokens))</span><br><span class="line">    filtered_tokens = [w <span class="keyword">for</span> w <span class="keyword">in</span> tokens <span class="keyword">if</span> re.search(<span class="string">'[a-zA-Z]'</span>, w)]</span><br><span class="line">    <span class="comment"># 去掉长度小于3的单词</span></span><br><span class="line">    filtered_tokens = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> filtered_tokens <span class="keyword">if</span> len(w)&gt;=<span class="number">3</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(filtered_tokens)</span><br><span class="line">    </span><br><span class="line">df[<span class="string">'tokens'</span>] = df[<span class="string">'item_description'</span>].map(tokenize)</span><br></pre></td></tr></table></figure>

<p>这里我们通过tokenize函数将<strong>item_description</strong>特征做了一些清洗，去掉了一些我们认为没用的信息。但是通过对比实验，我们发现新的<strong>tokens</strong>特征并没有原来的<strong>item_description</strong>特征好，所以我们放弃了这个处理，保持原来的特征。</p>
<h5 id="2-清洗奇异值"><a href="#2-清洗奇异值" class="headerlink" title="2. 清洗奇异值"></a>2. 清洗奇异值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = data[<span class="string">'price'</span>].mean()</span><br><span class="line">std = data[<span class="string">'price'</span>].std()</span><br><span class="line">error = data[np.abs(data[<span class="string">'price'</span>]-m) &gt; <span class="number">3</span>*std]</span><br><span class="line">correct = data[np.abs(data[<span class="string">'price'</span>]-m) &lt;= <span class="number">3</span>*std]</span><br><span class="line">data = data.drop(error,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>这样做得到的data去掉了一部分偏差比较大的价格，最终得到的结果比现在的结果上升了0.01，但是后来我们发现这种做法并不可取，因为如果这样处理了，通过留出法划分出来的测试集和训练集分布会更相似，而真正要预测的data分布就不一样了，所以我们也舍弃了这种处理方式。</p>
<h5 id="3-Tfidf与Count"><a href="#3-Tfidf与Count" class="headerlink" title="3. Tfidf与Count"></a>3. Tfidf与Count</h5><p>最开始<strong>name</strong>这个特征我们用的是CountVectorizer来将其转变为词频矩阵，但是我们后来发现，<strong>name</strong>中可能存在出现频率大但却不重要的词，所以我们将其乘上一个idf权重，即我们把CountVectorizer变成TfidfVectorizer，经过测试，确实TfidfVectorizer让结果优化了不少。</p>
<p><img alt data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/29.png" class="lozad"></p>
<p>这是我们最终的特征组合，其中分类的特征用的是CountVectorizer，而对于句子，描述等特称我们用的是TfidfVectorizer。我们还分别测试了ngram_range以及max_features不同的值对结果的影响，最终确定了以上的参数为我们的最优化模型。这也导致我们的模型太大，特征维度太高。</p>
<h3 id="七-调用的库"><a href="#七-调用的库" class="headerlink" title="七. 调用的库"></a>七. 调用的库</h3><table>
<thead>
<tr>
<th>运行环境</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Python</td>
<td>3.7.4</td>
</tr>
<tr>
<td>Keras</td>
<td>2.3.1</td>
</tr>
<tr>
<td>lightgbm</td>
<td>2.3.0</td>
</tr>
<tr>
<td>scipy</td>
<td>1.3.1</td>
</tr>
<tr>
<td>numpy</td>
<td>1.16.5</td>
</tr>
<tr>
<td>pandas</td>
<td>0.25.1</td>
</tr>
<tr>
<td>pillow</td>
<td>6.2.1</td>
</tr>
<tr>
<td>seaborn</td>
<td>0.9.0</td>
</tr>
<tr>
<td>matplotlib</td>
<td>3.1.1</td>
</tr>
<tr>
<td>scikit-learn</td>
<td>0.21.3</td>
</tr>
</tbody></table>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/01/15/data-science-5/">http://yoursite.com/2020/01/15/data-science-5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">阿瑜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/数据科学/">数据科学    </a><a class="post-meta__tags" href="/tags/数据分析/">数据分析    </a><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a><a class="post-meta__tags" href="/tags/回归/">回归    </a><a class="post-meta__tags" href="/tags/数据挖掘/">数据挖掘    </a><a class="post-meta__tags" href="/tags/神经网络/">神经网络    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/5/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/15/data-science-3/"><img class="prev_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/3/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>线性回归和逻辑回归</span></div></a></div><div class="next-post pull-right"><a href="/2019/09/20/hello-world/"><img class="next_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@2.0/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Hello World</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/01/15/data-science-3/" title="线性回归和逻辑回归"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/3/cover.jpg"><div class="relatedPosts_title">线性回归和逻辑回归</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-4/" title="梯度下降法"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/4/cover.jpg"><div class="relatedPosts_title">梯度下降法</div></a></div><div class="relatedPosts_item"><a href="/2020/01/15/data-science-2/" title="房屋数据处理分析"><img class="relatedPosts_cover lozad" data-src="https://cdn.jsdelivr.net/gh/darkseidddddd/CDN@1.0/DataScience/2/cover.jpg"><div class="relatedPosts_title">房屋数据处理分析</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div id="footer"><div class="copyright">&copy;2018 - 2020 By Yu</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Welcome to my <a href="https://darkseidddddd.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></section><div class=" " id="post_bottom"><div id="post_bottom_items"><a id="mobile_to_comment" href="#post-comment"><i class="mobile_scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#背景："><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">背景：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#分析目标："><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">分析目标：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#数据字段分析"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">数据字段分析</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#数据集"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#评价指标"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">评价指标</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#一-数据集"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">一. 数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#二-数据分析及预处理"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">二. 数据分析及预处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#三-特征选择"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">三. 特征选择</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#四-模型构建"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text">四. 模型构建</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-岭回归模型"><span class="toc_mobile_items-number">9.1.</span> <span class="toc_mobile_items-text">1. 岭回归模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-神经网络模型"><span class="toc_mobile_items-number">9.2.</span> <span class="toc_mobile_items-text">2. 神经网络模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-LightGBM"><span class="toc_mobile_items-number">9.3.</span> <span class="toc_mobile_items-text">3. LightGBM</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#五-预测及结果"><span class="toc_mobile_items-number">10.</span> <span class="toc_mobile_items-text">五. 预测及结果</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-岭回归模型-1"><span class="toc_mobile_items-number">10.1.</span> <span class="toc_mobile_items-text">1. 岭回归模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-神经网络模型-1"><span class="toc_mobile_items-number">10.2.</span> <span class="toc_mobile_items-text">2. 神经网络模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-lightGBM"><span class="toc_mobile_items-number">10.3.</span> <span class="toc_mobile_items-text">3. lightGBM</span></a></li></ol></li></ol><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#六-遇到的问题"><span class="toc_mobile_items-number"></span> <span class="toc_mobile_items-text">六. 遇到的问题</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#1-清洗item-description"><span class="toc_mobile_items-number">0.1.</span> <span class="toc_mobile_items-text">1. 清洗item_description</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#2-清洗奇异值"><span class="toc_mobile_items-number">0.2.</span> <span class="toc_mobile_items-text">2. 清洗奇异值</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#3-Tfidf与Count"><span class="toc_mobile_items-number">0.3.</span> <span class="toc_mobile_items-text">3. Tfidf与Count</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#七-调用的库"><span class="toc_mobile_items-number"></span> <span class="toc_mobile_items-text">七. 调用的库</span></a></li></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>